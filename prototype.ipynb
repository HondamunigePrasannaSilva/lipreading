{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#from torchtext.legacy.datasets import Multi30k\n",
    "#from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "#import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        #self.embedding = nn.Embedding(input_dim, emb_dim) FOR NOW WE DON't HAVE INPUT EMBEDDINGS\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=68*3, hidden_size=128, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #The input to the encoder are the landmarks\n",
    "        #x = [batch size, sequence_len , 68*3]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(src))FOR NOW WE DON't HAVE INPUT EMBEDDINGS\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "\n",
    "        out, hid = self.rnn(x)\n",
    "        \n",
    "        #out = [batch size, src len, hid dim * n directions]\n",
    "        #hidd = [n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "\n",
    "        #print(\"ENCODER: hid.shape: \", hid.shape)\n",
    "        print(\"ENCODER: hid:\\n\", hid)\n",
    "        return hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        #self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(output_dim, emb_dim)FOR NOW WE DON't HAVE INPUT EMBEDDINGS\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=128, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(2*hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        #input = input.unsqueeze(0).unsqueeze(0)COMMENTED TO DEBUG\n",
    "\n",
    "        #print(\"DECODER: input.shape\", input.shape)\n",
    "        \n",
    "        #input = [batch size, 1]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, hidden = self.rnn(input.to(torch.float32), hidden)\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "\n",
    "        #print(\"DECODER: prediction.shape\", prediction.shape)\n",
    "        \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        #assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "        #    \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        #assert encoder.n_layers == decoder.n_layers, \\\n",
    "        #    \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = src.shape[0]#FIXME Forse qui non ho passato i landmark con la dimensione del batch in testa\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden = self.encoder(src)\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "        #print(\"SEQ2SEQ: hidden.shape: \", hidden.shape)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = torch.tensor(31).unsqueeze(0).unsqueeze(0).unsqueeze(0)#trg[0,:] 31 is the index of <sos>\n",
    "        #print(\"SEQ2SEQ: input.shape: \", input.shape)\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            #print(\"SEQ2SEQ-FOR: input.shape: \", input.shape)\n",
    "            #print(\"SEQ2SEQ-FOR: hidden.shape: \", hidden.shape)\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            #print(\"SEQ2SEQ-FOR: output.shape: \", output.shape)\n",
    "            \n",
    "            output = output.unsqueeze(0)#ADDED TO BE CHECKED\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(2) \n",
    "\n",
    "            input = top1.unsqueeze(0)\n",
    "            #print(\"SEQ2SEQ-FOR: top1.shape: \", top1.shape)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "vocabulary = ['-'] + list(string.ascii_lowercase) + ['.', '?', ',', '!'] + ['@', '#', ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '.',\n",
       " '?',\n",
       " ',',\n",
       " '!',\n",
       " '@',\n",
       " '#',\n",
       " ' ']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get landmark using vocadataset.py\n",
    "from data.vocaset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = vocadataset(\"train\", landmark=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she had your dark suit in greasy wash water all year.\n"
     ]
    }
   ],
   "source": [
    "landmark, labels = trainset[0]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@she had your dark suit in greasy wash water all year.#'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = '@'+labels+'#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>she had your dark suit in greasy wash water all year.\\n<eos>'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 68*3\n",
    "emb_dim = 0\n",
    "hid_dim = 128\n",
    "n_layers = 0\n",
    "dropout = 0\n",
    "\n",
    "\n",
    "#model = Encoder(input_dim, emb_dim, hid_dim, n_layers, dropout)\n",
    "\n",
    "reshaped_landmark = torch.reshape(landmark, (landmark.shape[0], landmark.shape[1]*landmark.shape[2]))\n",
    "start_landmark = torch.zeros(1, 68*3)\n",
    "stop_landmark = torch.ones(1, 68*3)\n",
    "\n",
    "final_landmarks = torch.cat((start_landmark, reshaped_landmark, stop_landmark), 0)\n",
    "\n",
    "#out = model(reshaped_landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from characters to indices\n",
    "char_to_index = {char: index for index, char in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-': 0,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 27,\n",
       " '?': 28,\n",
       " ',': 29,\n",
       " '!': 30,\n",
       " '@': 31,\n",
       " '#': 32,\n",
       " ' ': 33}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequence and target to indices\n",
    "#sequence_indices = [char_to_index[char] for char in sequence]\n",
    "target_indices = [char_to_index[char] for char in labels]\n",
    "target_tensor = torch.tensor(target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31,\n",
       " 19,\n",
       " 8,\n",
       " 5,\n",
       " 33,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 33,\n",
       " 25,\n",
       " 15,\n",
       " 21,\n",
       " 18,\n",
       " 33,\n",
       " 4,\n",
       " 1,\n",
       " 18,\n",
       " 11,\n",
       " 33,\n",
       " 19,\n",
       " 21,\n",
       " 9,\n",
       " 20,\n",
       " 33,\n",
       " 9,\n",
       " 14,\n",
       " 33,\n",
       " 7,\n",
       " 18,\n",
       " 5,\n",
       " 1,\n",
       " 19,\n",
       " 25,\n",
       " 33,\n",
       " 23,\n",
       " 1,\n",
       " 19,\n",
       " 8,\n",
       " 33,\n",
       " 23,\n",
       " 1,\n",
       " 20,\n",
       " 5,\n",
       " 18,\n",
       " 33,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 33,\n",
       " 25,\n",
       " 5,\n",
       " 1,\n",
       " 18,\n",
       " 27,\n",
       " 32]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(vocabulary)\n",
    "\n",
    "enc = Encoder(input_dim, emb_dim, hid_dim, n_layers, dropout)\n",
    "dec = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout)\n",
    "\n",
    "model = Seq2Seq(enc, dec, 'cpu')#.to(device)\n",
    "\n",
    "reshaped_landmark = torch.reshape(landmark, (landmark.shape[0], landmark.shape[1]*landmark.shape[2]))\n",
    "start_landmark = torch.zeros(1, 68*3)\n",
    "stop_landmark = torch.ones(1, 68*3)\n",
    "\n",
    "final_landmarks = torch.cat((start_landmark, reshaped_landmark, stop_landmark), 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER: hid:\n",
      " tensor([[-1.6063e-01, -6.9836e-02, -1.1997e-01, -2.8645e-02,  1.3352e-01,\n",
      "          1.6212e-01,  1.2018e-01,  2.7975e-01,  1.2428e-01, -4.5522e-02,\n",
      "         -1.6396e-01,  1.6925e-01, -3.5751e-02,  5.3424e-02,  2.1607e-03,\n",
      "         -2.9854e-03, -8.8274e-02,  2.3255e-01, -6.0750e-02,  2.7966e-01,\n",
      "          7.7010e-02,  2.5489e-01, -2.0631e-02,  1.4633e-01, -1.3169e-01,\n",
      "          1.4802e-01, -6.8587e-02, -9.7953e-02,  1.2080e-01,  6.4370e-02,\n",
      "          8.2585e-02, -1.4442e-01, -1.4885e-02,  8.3471e-02, -5.9720e-02,\n",
      "          2.4837e-02, -4.1003e-02, -3.6045e-02,  9.9979e-02, -9.3806e-02,\n",
      "         -8.9514e-02, -2.1441e-01, -1.6406e-01, -4.9183e-03,  4.3422e-02,\n",
      "          6.6367e-02,  2.9248e-01,  6.6438e-02,  7.1486e-03, -1.7569e-01,\n",
      "          9.5391e-02, -5.6145e-02, -5.5782e-03, -1.3999e-02, -3.0993e-01,\n",
      "         -2.4762e-02,  4.8771e-02, -3.1594e-02, -4.1744e-02, -2.3525e-02,\n",
      "         -2.3924e-02, -9.2270e-02,  2.1891e-02,  2.0566e-01,  1.1514e-01,\n",
      "         -8.5894e-02, -2.8146e-02,  7.0738e-03,  3.7499e-02, -3.2616e-02,\n",
      "         -2.4579e-02,  4.8517e-02, -8.6540e-02,  4.1085e-02, -1.4247e-01,\n",
      "         -1.2857e-02, -6.9554e-02, -1.4191e-01, -5.2722e-02,  1.1171e-02,\n",
      "          6.3324e-02, -3.6696e-02, -1.4378e-01,  4.1359e-02,  8.7410e-03,\n",
      "          7.6492e-02,  1.2820e-01, -1.1845e-01,  6.9371e-03, -2.0974e-01,\n",
      "          1.9190e-01, -1.4591e-02, -5.6392e-02,  4.2808e-02, -3.0728e-01,\n",
      "         -7.8257e-02, -1.4484e-01, -3.3778e-02,  1.8861e-02,  9.8592e-02,\n",
      "         -1.0542e-01, -9.4404e-02,  1.1636e-02,  8.2640e-02,  8.4024e-02,\n",
      "         -3.3908e-02, -7.0729e-02, -5.3619e-02,  5.9467e-02, -5.3374e-02,\n",
      "          4.7401e-02, -1.4930e-01, -1.9438e-01, -5.7633e-02, -1.3887e-01,\n",
      "         -1.3109e-01, -2.3113e-02, -1.3738e-01, -5.1349e-02, -1.7079e-02,\n",
      "         -2.6847e-01,  8.2156e-02, -2.1848e-02,  6.4533e-02, -3.2133e-02,\n",
      "          2.4433e-01, -5.0612e-02, -2.4769e-02],\n",
      "        [-1.1748e-01,  8.3129e-02,  6.2710e-03,  8.3228e-02, -1.0941e-02,\n",
      "         -1.3568e-01, -1.6759e-01, -4.4853e-02,  1.6291e-01,  3.7178e-03,\n",
      "         -6.6624e-02,  4.1829e-02, -3.6399e-02,  1.4899e-01,  1.5462e-01,\n",
      "         -1.3095e-01,  1.0385e-01,  5.8292e-02, -4.7255e-02, -6.0448e-02,\n",
      "         -2.0349e-02,  1.7631e-01,  1.3543e-01, -2.4080e-02,  8.9212e-02,\n",
      "         -1.4091e-01, -3.1959e-02,  5.7744e-02, -2.2728e-02,  4.4320e-02,\n",
      "          5.1054e-02,  1.4267e-01,  1.7885e-01,  2.5054e-02, -1.4972e-01,\n",
      "         -1.2512e-02, -1.5879e-01, -2.7840e-02, -1.2637e-01, -7.0919e-02,\n",
      "          1.4253e-01,  7.9353e-02, -2.1448e-02,  2.2685e-01,  7.2068e-02,\n",
      "         -4.0312e-02, -1.7575e-01, -7.6249e-02,  2.4199e-03, -2.1417e-01,\n",
      "         -9.8495e-02,  1.5911e-01, -9.1600e-02,  2.3127e-02,  2.9986e-02,\n",
      "         -1.1649e-01, -1.0436e-01, -1.4748e-01, -5.3306e-02, -2.2759e-01,\n",
      "          2.7181e-01,  6.1568e-02,  1.2135e-02,  1.1179e-01, -1.4392e-01,\n",
      "         -1.4820e-01, -5.7603e-02, -9.4152e-02,  3.6768e-02,  1.3759e-03,\n",
      "          1.2191e-01,  9.0970e-02,  2.9728e-02,  4.7792e-02,  1.7184e-02,\n",
      "         -2.8788e-04, -3.4967e-02, -9.1028e-02, -2.2226e-01,  4.5441e-02,\n",
      "         -8.2593e-03,  7.2484e-02,  1.4211e-01,  1.7581e-01, -2.8481e-01,\n",
      "          1.3012e-01,  1.1425e-01, -8.4541e-02, -1.9548e-01,  5.2276e-02,\n",
      "         -4.0702e-02,  3.9343e-02,  1.7220e-02, -1.3783e-01, -1.4895e-01,\n",
      "         -9.8984e-03, -1.0583e-01, -6.4077e-02,  1.1330e-01,  1.1562e-01,\n",
      "          4.2504e-02, -6.2628e-02, -1.6178e-01,  8.9188e-02, -1.6155e-01,\n",
      "         -5.4134e-02, -1.6078e-01, -6.8658e-02,  1.0841e-01,  7.3979e-02,\n",
      "          5.0394e-02,  1.3895e-01,  2.4529e-02, -7.8536e-03, -4.7391e-02,\n",
      "          1.7794e-01,  1.6323e-01,  8.5273e-02, -2.6371e-01, -3.3777e-02,\n",
      "         -1.0567e-01,  2.2189e-02, -6.7369e-02,  9.4390e-02,  1.2721e-01,\n",
      "         -3.1506e-02,  1.7088e-01,  2.0600e-02]], grad_fn=<SqueezeBackward1>)\n",
      "-ekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekekeke\n",
      "ENCODER: hid:\n",
      " tensor([[-1.3553e-01, -7.1188e-02, -7.9965e-02, -7.8795e-02,  1.0940e-01,\n",
      "          1.9085e-01,  8.9682e-02,  3.3480e-01,  1.1022e-01, -1.7920e-02,\n",
      "         -1.1903e-01,  1.7257e-01, -8.1028e-02,  7.8664e-02, -3.4183e-02,\n",
      "          5.2581e-02, -5.9347e-02,  2.8720e-01, -1.6982e-02,  2.6647e-01,\n",
      "          1.5000e-01,  2.0063e-01, -9.8613e-02,  1.1278e-01, -1.8165e-01,\n",
      "          1.9598e-01, -5.1435e-02, -3.9152e-02,  1.5270e-01,  1.0693e-01,\n",
      "          1.2263e-01, -1.5739e-01, -3.5727e-02,  8.0668e-02, -1.8671e-02,\n",
      "         -8.0802e-03, -7.0523e-02, -2.2738e-02,  6.9619e-02, -1.0767e-01,\n",
      "         -1.3173e-01, -2.3257e-01, -1.3483e-01, -3.4339e-02,  1.1680e-01,\n",
      "          1.1519e-01,  2.7446e-01,  1.8796e-02, -2.1114e-03, -1.4345e-01,\n",
      "          7.0598e-02, -9.0053e-02,  1.2821e-02, -1.4729e-02, -2.9773e-01,\n",
      "          5.2889e-02,  3.4361e-02, -6.8704e-02, -6.7121e-02,  5.4795e-02,\n",
      "         -4.4781e-02, -3.6944e-02,  4.8187e-03,  2.1380e-01,  6.3304e-02,\n",
      "         -5.8757e-02, -8.8597e-02, -9.6334e-03,  8.3730e-02, -7.3490e-02,\n",
      "          1.0625e-02,  1.1988e-01, -1.2971e-01, -5.6702e-02, -1.3764e-01,\n",
      "         -4.1495e-02, -6.4066e-02, -9.9767e-02, -3.1368e-02,  9.2898e-02,\n",
      "          1.2789e-01, -7.4007e-05, -1.5688e-01,  1.4688e-02, -1.1344e-02,\n",
      "          1.0539e-01,  8.0649e-02, -8.4226e-02,  3.2283e-02, -2.0234e-01,\n",
      "          1.8310e-01, -5.0328e-03, -5.6733e-02,  9.7234e-02, -3.0304e-01,\n",
      "         -1.1652e-01, -1.5840e-01, -8.7620e-02,  2.4332e-02,  1.1970e-01,\n",
      "         -7.2373e-02, -1.3995e-01,  7.6987e-04,  9.4891e-02,  5.1556e-02,\n",
      "          2.5842e-02, -7.1562e-02, -6.1945e-02, -2.0608e-02, -3.9489e-02,\n",
      "          9.2004e-02, -1.4622e-01, -2.5378e-01, -3.2739e-02, -8.8719e-02,\n",
      "         -7.6920e-02, -6.7365e-02, -1.9865e-01, -9.0616e-02,  5.4485e-02,\n",
      "         -2.9603e-01,  7.9988e-02, -4.2915e-02,  1.1112e-02, -8.6672e-02,\n",
      "          2.3997e-01, -2.1632e-02,  3.7811e-03],\n",
      "        [-1.7108e-01,  7.8432e-02, -4.7758e-02,  1.2184e-01, -1.8877e-02,\n",
      "         -1.5218e-01, -1.4540e-01, -3.9432e-03,  1.2385e-01,  6.8647e-02,\n",
      "         -5.5990e-02,  2.1978e-02,  9.5966e-04,  1.5232e-01,  1.4876e-01,\n",
      "         -1.3249e-01,  2.1163e-01,  1.9707e-02, -4.4086e-02, -8.8099e-02,\n",
      "         -1.0351e-01,  1.4857e-01,  1.0359e-01,  8.6279e-03,  4.4186e-02,\n",
      "         -1.3414e-01, -1.1365e-02,  1.6932e-02, -2.9056e-02, -2.1919e-02,\n",
      "         -2.3015e-02,  1.5165e-01,  1.5484e-01,  4.1978e-02, -2.3859e-01,\n",
      "         -3.7331e-02, -2.0427e-01, -5.6381e-02, -1.7687e-01, -3.6238e-02,\n",
      "          1.1912e-01,  2.9272e-02, -3.9101e-02,  1.5931e-01,  1.2339e-01,\n",
      "         -6.4529e-02, -2.0539e-01, -5.8165e-02,  6.6977e-02, -2.3535e-01,\n",
      "         -1.4261e-01,  1.6134e-01, -2.0293e-02,  3.2411e-03,  5.2646e-02,\n",
      "         -1.3895e-01, -1.6318e-01, -2.1447e-01,  9.0978e-03, -1.6208e-01,\n",
      "          2.5532e-01,  2.1514e-02,  3.6060e-02,  1.3708e-01, -1.2826e-01,\n",
      "         -1.6551e-01, -1.2433e-01, -6.8681e-02,  7.1972e-02,  4.4218e-02,\n",
      "          1.1077e-01,  1.0484e-01,  6.9032e-02,  4.7859e-02, -6.0885e-03,\n",
      "         -4.2383e-02,  1.4505e-02, -5.5502e-02, -1.8625e-01,  3.6540e-02,\n",
      "          1.6086e-02,  5.9081e-03,  1.4226e-01,  1.8969e-01, -3.1392e-01,\n",
      "          1.7591e-01,  1.6517e-01, -1.2046e-01, -1.9708e-01,  2.3057e-02,\n",
      "         -1.1045e-02,  5.2180e-02,  7.7481e-03, -1.6842e-01, -1.0167e-01,\n",
      "          2.9659e-02, -1.5840e-01, -1.1374e-02,  1.0002e-01,  1.4008e-01,\n",
      "          2.0780e-02, -5.3929e-02, -2.0155e-01,  1.3545e-01, -1.7981e-01,\n",
      "         -6.9240e-02, -2.0499e-01, -1.0363e-01,  4.8966e-02,  2.9542e-02,\n",
      "          1.2388e-02,  1.0462e-01,  4.4712e-03,  1.8138e-02, -2.3353e-02,\n",
      "          1.7146e-01,  1.4464e-01,  1.3050e-01, -3.2459e-01,  1.5797e-02,\n",
      "         -8.1566e-02, -4.8810e-02, -1.0748e-02,  1.0091e-01,  9.9853e-02,\n",
      "          2.0967e-02,  1.8170e-01,  7.9225e-02]], grad_fn=<SqueezeBackward1>)\n",
      "-ekikkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\n",
      "ENCODER: hid:\n",
      " tensor([[-0.1201, -0.0706, -0.0368, -0.1338,  0.0876,  0.2195,  0.0584,  0.3887,\n",
      "          0.0984,  0.0017, -0.0719,  0.1781, -0.1276,  0.1011, -0.0738,  0.1141,\n",
      "         -0.0221,  0.3480,  0.0229,  0.2478,  0.2054,  0.1908, -0.1705,  0.0823,\n",
      "         -0.2459,  0.2441, -0.0327,  0.0159,  0.1874,  0.1568,  0.1545, -0.1696,\n",
      "         -0.0561,  0.0855,  0.0178, -0.0309, -0.0955,  0.0019,  0.0376, -0.1231,\n",
      "         -0.1676, -0.2611, -0.1066, -0.0646,  0.1975,  0.1520,  0.2992, -0.0308,\n",
      "          0.0250, -0.1209,  0.0575, -0.1193, -0.0022, -0.0102, -0.2742,  0.1262,\n",
      "          0.0253, -0.1089, -0.0931,  0.1255, -0.0743,  0.0186, -0.0024,  0.2329,\n",
      "          0.0097, -0.0297, -0.1525, -0.0319,  0.1261, -0.1204,  0.0391,  0.1992,\n",
      "         -0.1718, -0.1378, -0.1301, -0.0625, -0.0576, -0.0616, -0.0092,  0.1698,\n",
      "          0.1472,  0.0375, -0.1734, -0.0145, -0.0293,  0.1366,  0.0309, -0.0476,\n",
      "          0.0665, -0.2015,  0.1720,  0.0135, -0.0535,  0.1568, -0.3045, -0.1542,\n",
      "         -0.1765, -0.1523,  0.0407,  0.1521, -0.0357, -0.1812, -0.0073,  0.1222,\n",
      "          0.0237,  0.0763, -0.0743, -0.0800, -0.0803, -0.0208,  0.1088, -0.1839,\n",
      "         -0.3089, -0.0054, -0.0330, -0.0265, -0.1097, -0.2556, -0.1267,  0.1248,\n",
      "         -0.3303,  0.0808, -0.0609, -0.0395, -0.1397,  0.2289,  0.0103,  0.0365],\n",
      "        [-0.2243,  0.0745, -0.1083,  0.1769, -0.0096, -0.1803, -0.1341,  0.0370,\n",
      "          0.0675,  0.1490, -0.0657, -0.0005,  0.0517,  0.1649,  0.1360, -0.1342,\n",
      "          0.3250, -0.0237, -0.0186, -0.1143, -0.1902,  0.1566,  0.1128,  0.0280,\n",
      "          0.0016, -0.1708, -0.0114, -0.0354, -0.0327, -0.0687, -0.0788,  0.1745,\n",
      "          0.1320,  0.0778, -0.3167, -0.0793, -0.2370, -0.1007, -0.2237, -0.0058,\n",
      "          0.1152, -0.0015, -0.0611,  0.1050,  0.1682, -0.0874, -0.2540, -0.0186,\n",
      "          0.1307, -0.2642, -0.2096,  0.1952,  0.0613, -0.0066,  0.0665, -0.1824,\n",
      "         -0.2260, -0.2944,  0.0737, -0.0898,  0.2441,  0.0343,  0.1144,  0.1766,\n",
      "         -0.1240, -0.1949, -0.2022, -0.0393,  0.1025,  0.0929,  0.1069,  0.1201,\n",
      "          0.0871,  0.0516, -0.0451, -0.0625,  0.0637, -0.0610, -0.1905,  0.0070,\n",
      "          0.0454, -0.0621,  0.1813,  0.2128, -0.3583,  0.2308,  0.2027, -0.1568,\n",
      "         -0.2041, -0.0133,  0.0249,  0.0812, -0.0162, -0.2106, -0.0548,  0.0537,\n",
      "         -0.2096,  0.0174,  0.0879,  0.1747,  0.0506, -0.0597, -0.2249,  0.1910,\n",
      "         -0.1966, -0.0391, -0.2647, -0.1429,  0.0197,  0.0352, -0.0280,  0.0624,\n",
      "          0.0353,  0.0476,  0.0105,  0.1706,  0.1455,  0.1726, -0.4043,  0.0679,\n",
      "         -0.0736, -0.1107,  0.0514,  0.1117,  0.0881,  0.0713,  0.1567,  0.1507]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki?eki\n",
      "ENCODER: hid:\n",
      " tensor([[-0.1082, -0.0704,  0.0141, -0.1978,  0.0690,  0.2583,  0.0237,  0.4422,\n",
      "          0.0909,  0.0156, -0.0223,  0.1823, -0.1819,  0.1149, -0.1187,  0.1812,\n",
      "          0.0245,  0.4157,  0.0629,  0.2245,  0.2578,  0.1977, -0.2479,  0.0534,\n",
      "         -0.3252,  0.2960, -0.0107,  0.0652,  0.2309,  0.2162,  0.1788, -0.1864,\n",
      "         -0.0773,  0.0908,  0.0571, -0.0478, -0.1208,  0.0360,  0.0103, -0.1411,\n",
      "         -0.2072, -0.3004, -0.0792, -0.0995,  0.2869,  0.1691,  0.3386, -0.0759,\n",
      "          0.0569, -0.0979,  0.0440, -0.1547, -0.0373, -0.0039, -0.2412,  0.2070,\n",
      "          0.0150, -0.1531, -0.1236,  0.1990, -0.1297,  0.0714, -0.0026,  0.2587,\n",
      "         -0.0467,  0.0033, -0.2287, -0.0559,  0.1660, -0.1769,  0.0713,  0.2923,\n",
      "         -0.2213, -0.2200, -0.1321, -0.0860, -0.0460, -0.0194,  0.0190,  0.2504,\n",
      "          0.1538,  0.0883, -0.1945, -0.0475, -0.0448,  0.1756, -0.0240, -0.0145,\n",
      "          0.1038, -0.2040,  0.1564,  0.0389, -0.0481,  0.2230, -0.3095, -0.1983,\n",
      "         -0.2044, -0.2264,  0.0651,  0.1956,  0.0070, -0.2247, -0.0119,  0.1572,\n",
      "         -0.0007,  0.1273, -0.0944, -0.1029, -0.1228,  0.0063,  0.1246, -0.2440,\n",
      "         -0.3683,  0.0183,  0.0250,  0.0265, -0.1506, -0.3185, -0.1725,  0.1982,\n",
      "         -0.3731,  0.0860, -0.0800, -0.0961, -0.2008,  0.2303,  0.0424,  0.0770],\n",
      "        [-0.2789,  0.0891, -0.1791,  0.2517,  0.0051, -0.2191, -0.1315,  0.0818,\n",
      "          0.0028,  0.2413, -0.0928, -0.0204,  0.1126,  0.1881,  0.1228, -0.1439,\n",
      "          0.4427, -0.0788,  0.0234, -0.1484, -0.2778,  0.1863,  0.1408,  0.0438,\n",
      "         -0.0348, -0.2220, -0.0186, -0.0994, -0.0372, -0.0837, -0.1261,  0.2170,\n",
      "          0.1121,  0.1320, -0.3918, -0.1375, -0.2613, -0.1641, -0.2775,  0.0183,\n",
      "          0.1226, -0.0406, -0.0909,  0.0643,  0.2203, -0.1170, -0.3166,  0.0363,\n",
      "          0.2113, -0.2999, -0.2950,  0.2358,  0.1453, -0.0119,  0.0848, -0.2346,\n",
      "         -0.2922, -0.3842,  0.1494, -0.0140,  0.2333,  0.0670,  0.2143,  0.2291,\n",
      "         -0.1308, -0.2386, -0.2940, -0.0057,  0.1360,  0.1494,  0.1088,  0.1349,\n",
      "          0.0770,  0.0697, -0.0939, -0.0825,  0.1163, -0.0968, -0.2163, -0.0342,\n",
      "          0.0826, -0.1462,  0.2323,  0.2475, -0.4065,  0.2955,  0.2517, -0.2060,\n",
      "         -0.2080, -0.0588,  0.0563,  0.1290, -0.0655, -0.2692, -0.0243,  0.0817,\n",
      "         -0.2685,  0.0358,  0.0714,  0.2144,  0.0945, -0.0742, -0.2429,  0.2580,\n",
      "         -0.2122,  0.0187, -0.3417, -0.1865,  0.0212,  0.0547, -0.0672,  0.0067,\n",
      "          0.0714,  0.0794,  0.0491,  0.1703,  0.1569,  0.2187, -0.4975,  0.1293,\n",
      "         -0.0641, -0.1644,  0.1240,  0.1436,  0.0981,  0.1305,  0.1113,  0.2338]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-eyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeye\n",
      "ENCODER: hid:\n",
      " tensor([[-0.0961, -0.0743,  0.0593, -0.2771,  0.0529,  0.3116, -0.0219,  0.4974,\n",
      "          0.0887,  0.0279,  0.0313,  0.1865, -0.2497,  0.1182, -0.1744,  0.2552,\n",
      "          0.0797,  0.4881,  0.1055,  0.1894,  0.3145,  0.2168, -0.3378,  0.0295,\n",
      "         -0.4195,  0.3551,  0.0105,  0.1054,  0.2861,  0.2880,  0.1884, -0.2098,\n",
      "         -0.1065,  0.0981,  0.1024, -0.0599, -0.1507,  0.0778, -0.0007, -0.1604,\n",
      "         -0.2541, -0.3540, -0.0494, -0.1411,  0.3883,  0.1694,  0.3950, -0.1154,\n",
      "          0.0928, -0.0728,  0.0250, -0.1998, -0.0865, -0.0010, -0.2008,  0.2974,\n",
      "          0.0020, -0.2045, -0.1604,  0.2769, -0.2048,  0.1250,  0.0082,  0.2902,\n",
      "         -0.1043,  0.0450, -0.3160, -0.0841,  0.2051, -0.2480,  0.1135,  0.4018,\n",
      "         -0.2828, -0.3066, -0.1474, -0.1145, -0.0280,  0.0318,  0.0520,  0.3366,\n",
      "          0.1547,  0.1608, -0.2227, -0.0758, -0.0571,  0.2300, -0.0917,  0.0143,\n",
      "          0.1511, -0.2084,  0.1225,  0.0756, -0.0353,  0.3010, -0.3191, -0.2533,\n",
      "         -0.2428, -0.3120,  0.0983,  0.2533,  0.0621, -0.2728, -0.0151,  0.2014,\n",
      "         -0.0245,  0.1793, -0.1287, -0.1345, -0.1561,  0.0429,  0.1429, -0.3180,\n",
      "         -0.4329,  0.0324,  0.0811,  0.0843, -0.1931, -0.3887, -0.2337,  0.2806,\n",
      "         -0.4279,  0.0957, -0.1006, -0.1639, -0.2713,  0.2496,  0.0752,  0.1289],\n",
      "        [-0.3417,  0.1261, -0.2656,  0.3472,  0.0271, -0.2668, -0.1429,  0.1335,\n",
      "         -0.0765,  0.3470, -0.1389, -0.0377,  0.1890,  0.2280,  0.1067, -0.1598,\n",
      "          0.5639, -0.1508,  0.0809, -0.1949, -0.3707,  0.2339,  0.1878,  0.0614,\n",
      "         -0.0636, -0.2875, -0.0310, -0.1782, -0.0396, -0.0740, -0.1747,  0.2788,\n",
      "          0.0914,  0.2080, -0.4685, -0.2132, -0.2820, -0.2477, -0.3375,  0.0322,\n",
      "          0.1377, -0.0909, -0.1335,  0.0321,  0.2855, -0.1555, -0.3969,  0.1108,\n",
      "          0.3099, -0.3448, -0.4004,  0.2884,  0.2316, -0.0127,  0.1108, -0.2995,\n",
      "         -0.3677, -0.4831,  0.2388,  0.0716,  0.2196,  0.1143,  0.3336,  0.2972,\n",
      "         -0.1491, -0.2982, -0.4004,  0.0350,  0.1753,  0.2169,  0.1117,  0.1534,\n",
      "          0.0549,  0.1040, -0.1554, -0.1065,  0.1757, -0.1545, -0.2636, -0.0798,\n",
      "          0.1309, -0.2522,  0.2932,  0.2972, -0.4615,  0.3762,  0.3176, -0.2687,\n",
      "         -0.2130, -0.1167,  0.0782,  0.1977, -0.1340, -0.3470, -0.0060,  0.1192,\n",
      "         -0.3377,  0.0465,  0.0448,  0.2630,  0.1535, -0.1028, -0.2594,  0.3361,\n",
      "         -0.2275,  0.0989, -0.4354, -0.2359,  0.0418,  0.0897, -0.1056, -0.0660,\n",
      "          0.1142,  0.1155,  0.0961,  0.1685,  0.1750,  0.2732, -0.6019,  0.2056,\n",
      "         -0.0484, -0.2165,  0.2099,  0.1988,  0.1232,  0.1977,  0.0464,  0.3312]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-eyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeye\n",
      "ENCODER: hid:\n",
      " tensor([[-0.0833, -0.0840,  0.0900, -0.3740,  0.0369,  0.3814, -0.0842,  0.5551,\n",
      "          0.0936,  0.0429,  0.0912,  0.1929, -0.3344,  0.1091, -0.2431,  0.3373,\n",
      "          0.1438,  0.5640,  0.1538,  0.1381,  0.3791,  0.2503, -0.4410,  0.0185,\n",
      "         -0.5256,  0.4225,  0.0306,  0.1359,  0.3550,  0.3727,  0.1807, -0.2440,\n",
      "         -0.1503,  0.1088,  0.1553, -0.0639, -0.1879,  0.1264,  0.0075, -0.1813,\n",
      "         -0.3128, -0.4250, -0.0125, -0.1931,  0.5024,  0.1560,  0.4700, -0.1489,\n",
      "          0.1339, -0.0431, -0.0020, -0.2557, -0.1508, -0.0009, -0.1555,  0.3968,\n",
      "         -0.0146, -0.2686, -0.2078,  0.3609, -0.2957,  0.1810,  0.0341,  0.3286,\n",
      "         -0.1618,  0.0977, -0.4136, -0.1187,  0.2460, -0.3353,  0.1703,  0.5245,\n",
      "         -0.3585, -0.3986, -0.1721, -0.1503, -0.0010,  0.0956,  0.0930,  0.4299,\n",
      "          0.1502,  0.2580, -0.2594, -0.0943, -0.0678,  0.3042, -0.1759,  0.0389,\n",
      "          0.2142, -0.2133,  0.0683,  0.1268, -0.0108,  0.3930, -0.3378, -0.3229,\n",
      "         -0.2937, -0.4120,  0.1415,  0.3279,  0.1348, -0.3269, -0.0204,  0.2583,\n",
      "         -0.0499,  0.2310, -0.1755, -0.1805, -0.1823,  0.0917,  0.1625, -0.4028,\n",
      "         -0.5029,  0.0327,  0.1349,  0.1512, -0.2402, -0.4665, -0.3127,  0.3739,\n",
      "         -0.4956,  0.1128, -0.1234, -0.2470, -0.3513,  0.2848,  0.1113,  0.1948],\n",
      "        [-0.4147,  0.1871, -0.3692,  0.4602,  0.0590, -0.3233, -0.1708,  0.1917,\n",
      "         -0.1767,  0.4638, -0.2075, -0.0501,  0.2836,  0.2888,  0.0834, -0.1757,\n",
      "          0.6784, -0.2417,  0.1553, -0.2560, -0.4660,  0.2994,  0.2553,  0.0852,\n",
      "         -0.0863, -0.3684, -0.0519, -0.2736, -0.0351, -0.0393, -0.2257,  0.3609,\n",
      "          0.0692,  0.3101, -0.5433, -0.3050, -0.3029, -0.3533, -0.4049,  0.0319,\n",
      "          0.1631, -0.1516, -0.1920,  0.0107,  0.3631, -0.2059, -0.4943,  0.2106,\n",
      "          0.4231, -0.4023, -0.5216,  0.3512,  0.3112, -0.0089,  0.1479, -0.3790,\n",
      "         -0.4534, -0.5861,  0.3428,  0.1676,  0.2002,  0.1810,  0.4666,  0.3843,\n",
      "         -0.1791, -0.3755, -0.5177,  0.0840,  0.2243,  0.2973,  0.1101,  0.1784,\n",
      "          0.0195,  0.1586, -0.2338, -0.1374,  0.2448, -0.2347, -0.3369, -0.1233,\n",
      "          0.1915, -0.3771,  0.3631,  0.3639, -0.5220,  0.4729,  0.3990, -0.3438,\n",
      "         -0.2234, -0.1915,  0.0955,  0.2923, -0.2211, -0.4441,  0.0020,  0.1694,\n",
      "         -0.4177,  0.0483,  0.0042,  0.3213,  0.2304, -0.1549, -0.2756,  0.4250,\n",
      "         -0.2442,  0.2023, -0.5414, -0.2920,  0.0838,  0.1465, -0.1364, -0.1556,\n",
      "          0.1685,  0.1584,  0.1564,  0.1612,  0.2014,  0.3404, -0.7088,  0.2982,\n",
      "         -0.0220, -0.2608,  0.3105,  0.2742,  0.1664,  0.2653, -0.0405,  0.4393]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-eyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeyeye\n",
      "ENCODER: hid:\n",
      " tensor([[-0.0716, -0.1038,  0.1031, -0.4843,  0.0175,  0.4661, -0.1671,  0.6146,\n",
      "          0.1072,  0.0654,  0.1609,  0.2036, -0.4362,  0.0865, -0.3222,  0.4263,\n",
      "          0.2184,  0.6402,  0.2129,  0.0696,  0.4522,  0.3016, -0.5513,  0.0220,\n",
      "         -0.6345,  0.4974,  0.0509,  0.1551,  0.4382,  0.4678,  0.1546, -0.2945,\n",
      "         -0.2139,  0.1215,  0.2176, -0.0556, -0.2354,  0.1798,  0.0356, -0.2051,\n",
      "         -0.3861, -0.5130,  0.0333, -0.2598,  0.6208,  0.1305,  0.5603, -0.1744,\n",
      "          0.1800, -0.0047, -0.0398, -0.3229, -0.2270, -0.0025, -0.1110,  0.5010,\n",
      "         -0.0363, -0.3471, -0.2719,  0.4496, -0.3975,  0.2416,  0.0771,  0.3758,\n",
      "         -0.2174,  0.1616, -0.5167, -0.1622,  0.2925, -0.4358,  0.2448,  0.6482,\n",
      "         -0.4484, -0.4950, -0.2049, -0.1985,  0.0364,  0.1746,  0.1458,  0.5284,\n",
      "          0.1415,  0.3761, -0.3042, -0.1005, -0.0813,  0.3962, -0.2764,  0.0601,\n",
      "          0.2954, -0.2167, -0.0098,  0.1936,  0.0300,  0.4956, -0.3688, -0.4079,\n",
      "         -0.3584, -0.5228,  0.1956,  0.4177,  0.2275, -0.3869, -0.0345,  0.3308,\n",
      "         -0.0781,  0.2815, -0.2332, -0.2434, -0.2005,  0.1540,  0.1859, -0.4941,\n",
      "         -0.5777,  0.0190,  0.1862,  0.2297, -0.2939, -0.5503, -0.4081,  0.4756,\n",
      "         -0.5720,  0.1396, -0.1491, -0.3456, -0.4374,  0.3352,  0.1530,  0.2771],\n",
      "        [-0.4946,  0.2701, -0.4825,  0.5783,  0.1027, -0.3849, -0.2122,  0.2551,\n",
      "         -0.2970,  0.5795, -0.2989, -0.0504,  0.3918,  0.3668,  0.0485, -0.1828,\n",
      "          0.7729, -0.3469,  0.2436, -0.3306, -0.5545,  0.3789,  0.3395,  0.1201,\n",
      "         -0.1011, -0.4585, -0.0887, -0.3815, -0.0209,  0.0235, -0.2770,  0.4577,\n",
      "          0.0462,  0.4344, -0.6057, -0.4064, -0.3283, -0.4733, -0.4785,  0.0161,\n",
      "          0.2034, -0.2206, -0.2670,  0.0082,  0.4437, -0.2674, -0.5997,  0.3299,\n",
      "          0.5381, -0.4735, -0.6438,  0.4118,  0.3724, -0.0052,  0.1989, -0.4687,\n",
      "         -0.5443, -0.6810,  0.4563,  0.2674,  0.1709,  0.2720,  0.5972,  0.4855,\n",
      "         -0.2171, -0.4651, -0.6328,  0.1414,  0.2864,  0.3874,  0.0965,  0.2129,\n",
      "         -0.0337,  0.2362, -0.3256, -0.1789,  0.3240, -0.3345, -0.4346, -0.1588,\n",
      "          0.2642, -0.5081,  0.4377,  0.4442, -0.5831,  0.5767,  0.4898, -0.4271,\n",
      "         -0.2427, -0.2835,  0.1121,  0.4109, -0.3231, -0.5522, -0.0022,  0.2329,\n",
      "         -0.5035,  0.0379, -0.0537,  0.3860,  0.3250, -0.2321, -0.2917,  0.5191,\n",
      "         -0.2642,  0.3245, -0.6466, -0.3542,  0.1504,  0.2274, -0.1500, -0.2521,\n",
      "          0.2370,  0.2091,  0.2320,  0.1430,  0.2358,  0.4214, -0.8023,  0.4021,\n",
      "          0.0194, -0.2907,  0.4196,  0.3661,  0.2307,  0.3223, -0.1491,  0.5450]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-eye e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e\n",
      "ENCODER: hid:\n",
      " tensor([[-0.0618, -0.1396,  0.0945, -0.5960, -0.0096,  0.5588, -0.2700,  0.6729,\n",
      "          0.1307,  0.0987,  0.2414,  0.2210, -0.5492,  0.0553, -0.4052,  0.5157,\n",
      "          0.3029,  0.7113,  0.2878, -0.0128,  0.5302,  0.3711, -0.6573,  0.0375,\n",
      "         -0.7322,  0.5768,  0.0748,  0.1601,  0.5305,  0.5656,  0.1109, -0.3641,\n",
      "         -0.3009,  0.1354,  0.2893, -0.0312, -0.2952,  0.2303,  0.0843, -0.2303,\n",
      "         -0.4725, -0.6096,  0.0882, -0.3436,  0.7267,  0.0956,  0.6555, -0.1856,\n",
      "          0.2307,  0.0487, -0.0932, -0.3990, -0.2944, -0.0066, -0.0693,  0.6018,\n",
      "         -0.0657, -0.4379, -0.3569,  0.5393, -0.5008,  0.3096,  0.1369,  0.4321,\n",
      "         -0.2683,  0.2352, -0.6154, -0.2151,  0.3464, -0.5405,  0.3369,  0.7557,\n",
      "         -0.5468, -0.5903, -0.2463, -0.2626,  0.0825,  0.2679,  0.2114,  0.6247,\n",
      "          0.1386,  0.5019, -0.3530, -0.0944, -0.1042,  0.4971, -0.3864,  0.0808,\n",
      "          0.3912, -0.2119, -0.1105,  0.2738,  0.0928,  0.5984, -0.4130, -0.5047,\n",
      "         -0.4337, -0.6328,  0.2585,  0.5148,  0.3367, -0.4508, -0.0677,  0.4167,\n",
      "         -0.1059,  0.3285, -0.3001, -0.3215, -0.2108,  0.2308,  0.2182, -0.5840,\n",
      "         -0.6527, -0.0089,  0.2318,  0.3187, -0.3546, -0.6329, -0.5125,  0.5787,\n",
      "         -0.6484,  0.1772, -0.1778, -0.4537, -0.5212,  0.4008,  0.2024,  0.3738],\n",
      "        [-0.5730,  0.3704, -0.5913,  0.6826,  0.1557, -0.4446, -0.2599,  0.3179,\n",
      "         -0.4252,  0.6785, -0.4048, -0.0319,  0.5013,  0.4518, -0.0018, -0.1713,\n",
      "          0.8402, -0.4535,  0.3344, -0.4131, -0.6258,  0.4634,  0.4310,  0.1696,\n",
      "         -0.1026, -0.5474, -0.1393, -0.4920, -0.0062,  0.1140, -0.3246,  0.5577,\n",
      "          0.0253,  0.5622, -0.6451, -0.5057, -0.3592, -0.5915, -0.5526, -0.0170,\n",
      "          0.2604, -0.2938, -0.3532,  0.0312,  0.5041, -0.3295, -0.6969,  0.4456,\n",
      "          0.6338, -0.5532, -0.7471,  0.4477,  0.4059, -0.0088,  0.2614, -0.5559,\n",
      "         -0.6305, -0.7569,  0.5662,  0.3596,  0.1294,  0.3830,  0.7071,  0.5873,\n",
      "         -0.2550, -0.5557, -0.7293,  0.2042,  0.3598,  0.4761,  0.0629,  0.2568,\n",
      "         -0.1072,  0.3343, -0.4189, -0.2323,  0.4102, -0.4422, -0.5429, -0.1776,\n",
      "          0.3461, -0.6266,  0.5103,  0.5288, -0.6390,  0.6719,  0.5802, -0.5119,\n",
      "         -0.2733, -0.3833,  0.1292,  0.5376, -0.4309, -0.6562, -0.0230,  0.3064,\n",
      "         -0.5854,  0.0110, -0.1280,  0.4505,  0.4280, -0.3279, -0.3065,  0.6075,\n",
      "         -0.2881,  0.4507, -0.7349, -0.4175,  0.2406,  0.3253, -0.1389, -0.3371,\n",
      "          0.3161,  0.2660,  0.3196,  0.1072,  0.2734,  0.5102, -0.8700,  0.5053,\n",
      "          0.0769, -0.2989,  0.5244,  0.4670,  0.3128,  0.3577, -0.2721,  0.6312]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-ey??                                                                                                                                                                                                                                               \n",
      "ENCODER: hid:\n",
      " tensor([[-4.9655e-02, -1.9537e-01,  6.2405e-02, -6.9481e-01, -4.7683e-02,\n",
      "          6.4892e-01, -3.8489e-01,  7.2704e-01,  1.6262e-01,  1.4374e-01,\n",
      "          3.3059e-01,  2.4327e-01, -6.5956e-01,  3.3039e-02, -4.8667e-01,\n",
      "          5.9724e-01,  3.9269e-01,  7.7115e-01,  3.7759e-01, -1.0371e-01,\n",
      "          6.0528e-01,  4.5264e-01, -7.4718e-01,  6.1210e-02, -8.0730e-01,\n",
      "          6.5449e-01,  1.0621e-01,  1.4849e-01,  6.2025e-01,  6.5661e-01,\n",
      "          5.2354e-02, -4.4804e-01, -4.0887e-01,  1.5512e-01,  3.6576e-01,\n",
      "          8.4390e-03, -3.6550e-01,  2.6840e-01,  1.5170e-01, -2.4596e-01,\n",
      "         -5.6420e-01, -7.0124e-01,  1.5185e-01, -4.4076e-01,  8.0797e-01,\n",
      "          5.4552e-02,  7.4224e-01, -1.7555e-01,  2.8724e-01,  1.2243e-01,\n",
      "         -1.6768e-01, -4.7696e-01, -3.3383e-01, -1.9065e-02, -2.9503e-02,\n",
      "          6.8978e-01, -1.0431e-01, -5.3452e-01, -4.5901e-01,  6.2342e-01,\n",
      "         -5.9493e-01,  3.8662e-01,  2.0856e-01,  4.9194e-01, -3.1250e-01,\n",
      "          3.1637e-01, -6.9961e-01, -2.7631e-01,  4.0683e-01, -6.3761e-01,\n",
      "          4.3930e-01,  8.3531e-01, -6.4258e-01, -6.7537e-01, -2.9425e-01,\n",
      "         -3.4021e-01,  1.3260e-01,  3.6884e-01,  2.7996e-01,  7.0808e-01,\n",
      "          1.5631e-01,  6.1661e-01, -3.9959e-01, -7.8455e-02, -1.4214e-01,\n",
      "          5.9330e-01, -4.9595e-01,  1.0511e-01,  4.9260e-01, -1.9040e-01,\n",
      "         -2.2672e-01,  3.6217e-01,  1.7935e-01,  6.8893e-01, -4.6544e-01,\n",
      "         -6.0380e-01, -5.1180e-01, -7.2817e-01,  3.2650e-01,  6.0867e-01,\n",
      "          4.5238e-01, -5.1291e-01, -1.2690e-01,  5.0888e-01, -1.2459e-01,\n",
      "          3.7127e-01, -3.7292e-01, -4.1007e-01, -2.1256e-01,  3.1975e-01,\n",
      "          2.5522e-01, -6.6310e-01, -7.2117e-01, -5.0733e-02,  2.6972e-01,\n",
      "          4.1176e-01, -4.1915e-01, -7.0460e-01, -6.1250e-01,  6.7270e-01,\n",
      "         -7.1572e-01,  2.2268e-01, -2.0867e-01, -5.5915e-01, -5.9356e-01,\n",
      "          4.7714e-01,  2.5718e-01,  4.7607e-01],\n",
      "        [-6.4391e-01,  4.7801e-01, -6.8394e-01,  7.6273e-01,  2.0984e-01,\n",
      "         -4.9606e-01, -3.0952e-01,  3.7493e-01, -5.4602e-01,  7.5364e-01,\n",
      "         -5.1158e-01,  1.1896e-03,  5.9925e-01,  5.3435e-01, -6.8123e-02,\n",
      "         -1.3620e-01,  8.8393e-01, -5.4813e-01,  4.0954e-01, -4.9595e-01,\n",
      "         -6.7696e-01,  5.4492e-01,  5.1838e-01,  2.3254e-01, -8.5953e-02,\n",
      "         -6.2704e-01, -1.9699e-01, -5.9574e-01, -2.7719e-04,  2.2368e-01,\n",
      "         -3.6522e-01,  6.4966e-01,  9.5246e-03,  6.7263e-01, -6.6264e-01,\n",
      "         -5.9339e-01, -3.9563e-01, -6.9276e-01, -6.2070e-01, -6.7688e-02,\n",
      "          3.2992e-01, -3.6850e-01, -4.4016e-01,  8.1028e-02,  5.3308e-01,\n",
      "         -3.7780e-01, -7.7391e-01,  5.3666e-01,  6.9777e-01, -6.3140e-01,\n",
      "         -8.2263e-01,  4.5709e-01,  4.1019e-01, -2.3034e-02,  3.2839e-01,\n",
      "         -6.2939e-01, -7.0474e-01, -8.1124e-01,  6.5933e-01,  4.3641e-01,\n",
      "          7.7435e-02,  4.9912e-01,  7.8969e-01,  6.7716e-01, -2.8595e-01,\n",
      "         -6.3757e-01, -8.0104e-01,  2.6873e-01,  4.3663e-01,  5.5520e-01,\n",
      "          5.5066e-03,  3.0939e-01, -1.9609e-01,  4.4230e-01, -5.0214e-01,\n",
      "         -2.9614e-01,  4.9798e-01, -5.4433e-01, -6.4521e-01, -1.7870e-01,\n",
      "          4.2967e-01, -7.2250e-01,  5.7535e-01,  6.0875e-01, -6.8917e-01,\n",
      "          7.4853e-01,  6.6293e-01, -5.9107e-01, -3.1694e-01, -4.7893e-01,\n",
      "          1.4751e-01,  6.5262e-01, -5.3701e-01, -7.4424e-01, -6.3578e-02,\n",
      "          3.8434e-01, -6.5540e-01, -3.3778e-02, -2.1573e-01,  5.1117e-01,\n",
      "          5.2762e-01, -4.3270e-01, -3.1915e-01,  6.8326e-01, -3.1595e-01,\n",
      "          5.6094e-01, -8.0103e-01, -4.7485e-01,  3.4477e-01,  4.2793e-01,\n",
      "         -1.0327e-01, -3.9786e-01,  3.9934e-01,  3.2757e-01,  4.1143e-01,\n",
      "          5.0604e-02,  3.0686e-01,  5.9764e-01, -9.1313e-01,  5.9701e-01,\n",
      "          1.4712e-01, -2.8508e-01,  6.1440e-01,  5.6617e-01,  4.0478e-01,\n",
      "          3.6484e-01, -3.9777e-01,  6.9140e-01]], grad_fn=<SqueezeBackward1>)\n",
      "-ey?                                                                                                                                                                                                                                                \n",
      "ENCODER: hid:\n",
      " tensor([[-0.0315, -0.2686,  0.0097, -0.7721, -0.0938,  0.7279, -0.4986,  0.7738,\n",
      "          0.1981,  0.1986,  0.4229,  0.2620, -0.7520,  0.0311, -0.5642,  0.6663,\n",
      "          0.4789,  0.8168,  0.4727, -0.1982,  0.6697,  0.5354, -0.8151,  0.0907,\n",
      "         -0.8587,  0.7237,  0.1484,  0.1210,  0.6960,  0.7329, -0.0164, -0.5354,\n",
      "         -0.5252,  0.1878,  0.4382,  0.0573, -0.4401,  0.2903,  0.2314, -0.2405,\n",
      "         -0.6490, -0.7774,  0.2246, -0.5409,  0.8636,  0.0079,  0.8117, -0.1402,\n",
      "          0.3518,  0.2161, -0.2623, -0.5486, -0.3449, -0.0460,  0.0122,  0.7594,\n",
      "         -0.1528, -0.6269, -0.5653,  0.6960, -0.6733,  0.4703,  0.2857,  0.5407,\n",
      "         -0.3506,  0.4014, -0.7649, -0.3435,  0.4696, -0.7186,  0.5399,  0.8874,\n",
      "         -0.7241, -0.7424, -0.3415, -0.4219,  0.1825,  0.4678,  0.3361,  0.7712,\n",
      "          0.1968,  0.7079, -0.4401, -0.0550, -0.1956,  0.6743, -0.5957,  0.1345,\n",
      "          0.5880, -0.1475, -0.3502,  0.4512,  0.2836,  0.7594, -0.5165, -0.6936,\n",
      "         -0.5838, -0.8009,  0.3959,  0.6899,  0.5615, -0.5677, -0.2087,  0.5969,\n",
      "         -0.1274,  0.4122, -0.4473, -0.5020, -0.2038,  0.4146,  0.2847, -0.7265,\n",
      "         -0.7777, -0.1047,  0.3026,  0.5002, -0.4805, -0.7603, -0.6966,  0.7490,\n",
      "         -0.7697,  0.2700, -0.2437, -0.6497, -0.6511,  0.5534,  0.3124,  0.5722],\n",
      "        [-0.7034,  0.5806, -0.7565,  0.8204,  0.2597, -0.5346, -0.3586,  0.4248,\n",
      "         -0.6478,  0.8070, -0.6070,  0.0362,  0.6792,  0.6081, -0.1476, -0.0772,\n",
      "          0.9113, -0.6239,  0.4634, -0.5735, -0.7102,  0.6180,  0.5948,  0.3065,\n",
      "         -0.0498, -0.6942, -0.2562, -0.6841, -0.0067,  0.3412, -0.3957,  0.7267,\n",
      "          0.0045,  0.7573, -0.6603, -0.6654, -0.4357, -0.7714, -0.6803, -0.1330,\n",
      "          0.4046, -0.4427, -0.5180,  0.1538,  0.5344, -0.4044, -0.8297,  0.6004,\n",
      "          0.7361, -0.7004, -0.8729,  0.4453,  0.3873, -0.0414,  0.3955, -0.6833,\n",
      "         -0.7640, -0.8479,  0.7314,  0.4969,  0.0175,  0.6047,  0.8478,  0.7488,\n",
      "         -0.3052, -0.7062, -0.8511,  0.3322,  0.5096,  0.6224, -0.0715,  0.3676,\n",
      "         -0.2910,  0.5479, -0.5700, -0.3668,  0.5803, -0.6326, -0.7310, -0.1720,\n",
      "          0.5060, -0.7943,  0.6305,  0.6785, -0.7337,  0.8055,  0.7335, -0.6600,\n",
      "         -0.3716, -0.5635,  0.1653,  0.7438, -0.6338, -0.8115, -0.1233,  0.4619,\n",
      "         -0.7123, -0.0938, -0.3110,  0.5657,  0.6155, -0.5348, -0.3260,  0.7444,\n",
      "         -0.3476,  0.6442, -0.8471, -0.5189,  0.4507,  0.5241, -0.0481, -0.4329,\n",
      "          0.4796,  0.3912,  0.4989, -0.0248,  0.3346,  0.6761, -0.9395,  0.6726,\n",
      "          0.2256, -0.2514,  0.6866,  0.6552,  0.4980,  0.3446, -0.5153,  0.7283]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "-ey                                                                                                                                                                                                                                                 \n",
      "Epoch [10/10], Loss: 4.20652961730957\n"
     ]
    }
   ],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(reshaped_landmark, target_tensor[None,:])\n",
    "\n",
    "    #output = output[None,:,:]\n",
    "    #output = output.permute(1, 0, 2)  # Swap batch and sequence dimensions\n",
    "    \n",
    "    e = torch.argmax(output, dim=2).squeeze(1)\n",
    "    output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "    print(output_sequence)\n",
    "\n",
    "    \n",
    "    input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "    target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.long)\n",
    "    \n",
    "    loss = ctc_loss(output, target_tensor, input_lengths, target_lengths[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([31, 19,  8,  5, 33,  8,  1,  4, 33, 25, 15, 21, 18, 33,  4,  1, 18, 11,\n",
       "        33, 19, 21,  9, 20, 33,  9, 14, 33,  7, 18,  5,  1, 19, 25, 33, 23,  1,\n",
       "        19,  8, 33, 23,  1, 20,  5, 18, 33,  1, 12, 12, 33, 25,  5,  1, 18, 27,\n",
       "        32])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([244, 1, 34])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output sequence\n",
    "output_indices = torch.argmax(output, dim=2).squeeze(1)\n",
    "output_sequence = ''.join([vocabulary[index] for index in output_indices])\n",
    "\n",
    "print(\"Original Sequence:\", sequence)\n",
    "print(\"Target Sequence:\", target)\n",
    "\n",
    "def process_string(input_string):\n",
    "    output_string = \"\"\n",
    "    current_char = \"\"\n",
    "\n",
    "    for char in input_string:\n",
    "        if char != current_char:\n",
    "            if char.isalpha() or char == '0':\n",
    "                if char == '0':\n",
    "                    output_string += ' '\n",
    "                else:\n",
    "                    output_string += char   \n",
    "            current_char = char\n",
    "\n",
    "    return output_string.strip()\n",
    "\n",
    "print(\"Decoded Output:\", process_string(output_sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipreading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
