{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype of lipreading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# model file, encoder, decoder and seqtoseq\n",
    "from model_temp import *\n",
    "# utils file\n",
    "from utils import *\n",
    "# Get landmark using vocadataset.py\n",
    "from data.vocaset import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get landmark from vocadaset class\n",
    "#trainset = vocadataset(\"train\", landmark=True)\n",
    "trainset = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "#landmark, labels = trainset[0]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=20, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "#landmarks, len_landmark, label, len_label = next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(blank=\"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from characters to indices\n",
    "char_to_index = {char: index for index, char in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequence and target to indices\n",
    "#sequence_indices = [char_to_index[char] for char in sequence]\n",
    "\n",
    "label_t = char_to_index_batch(label, vocabulary)\n",
    "\n",
    "#target_indices = [char_to_index[char] for char in labels]\n",
    "#target_tensor = torch.tensor(target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 36*3\n",
    "INPUT_DIM = 36*3\n",
    "HID_DIM = 64\n",
    "output_dim = len(vocabulary)\n",
    "\n",
    "model = only_Decoder2(INPUT_DIM, HID_DIM, 2, len(vocabulary)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.reshape(landmarks[0], (landmarks[0].shape[0], landmarks[0].shape[1]*landmarks[0].shape[2]))[None,:,:].shape)\n",
    "print(len_landmark[0][None, None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i in range(landmarks.shape[0]):\n",
    "        landmarks[i] = landmarks[i].to(device) \n",
    "        label_t[i] = label_t[i].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        target_tensor = label_t[i]\n",
    "\n",
    "        \n",
    "\n",
    "        reshaped_landmark = torch.reshape(landmarks[i], (landmarks[i].shape[0], landmarks[i].shape[1]*landmarks[i].shape[2]))\n",
    "        reshaped_landmark = reshaped_landmark.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        len_landmark[i] = len_landmark[i].to(device) \n",
    "        output = model(reshaped_landmark[None, : , :], len_landmark[i][None])\n",
    "        output = output.permute(1,0,2)\n",
    "        input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.int32)\n",
    "        \n",
    "        loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), target_tensor, input_lengths, target_lengths[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "        if(epoch + 1) % 100 == 0:\n",
    "            f = open(\"prova_.txt\", \"a\")\n",
    "            f.write(label[i]+\"\\n\")\n",
    "            f.write(output_sequence+\"\\n\")\n",
    "            f.close() \n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([269, 36, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw = landmarks[0][:len_landmark[0], :, :].clone().detach().requires_grad_(True)\n",
    "nw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_l = label_t[0][:len_label[0]].clone().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 28,  4, 19,  2,  3, 28,  4,  9,  2, 13, 13,  6, 15,  8,  6,  5, 28,\n",
       "        14,  6, 31, 28,  3, 22, 21, 28,  2, 28, 18, 22, 10,  4, 12, 28, 20, 21,\n",
       "         2,  3, 28, 23,  2, 15, 18, 22, 10, 20,  9,  6,  5, 28,  9, 10, 14, 29,\n",
       "        29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "for _ in range (16):\n",
    "    landmarks, len_landmark, label, len_label = next(iter(trainloader))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for i in range(landmarks.shape[0]):\n",
    "            landmarks[i] = landmarks[i].to(device) \n",
    "            label_t[i] = label_t[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            #remove padding from dataloader\n",
    "            nw = landmarks[i][:len_landmark[i], :, :].clone().detach().requires_grad_(True)\n",
    "            nw_l = label_t[i][:len_label[i]].clone().detach()\n",
    "            \n",
    "            target_tensor = nw_l\n",
    "            reshaped_landmark = torch.reshape(nw, (nw.shape[0], nw.shape[1]*nw.shape[2]))\n",
    "            reshaped_landmark = reshaped_landmark.to(device)\n",
    "            target_tensor = target_tensor.to(device)\n",
    "            len_landmark[i] = len_landmark[i].to(device) \n",
    "            output = model(reshaped_landmark[None, : , :], len_landmark[i][None])\n",
    "            output = output.permute(1,0,2)\n",
    "            input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "            target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.int32)\n",
    "            \n",
    "            loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), target_tensor, input_lengths, target_lengths[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            e = torch.argmax(output, dim=2).squeeze(1)\n",
    "            output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "            #print(output_sequence)\n",
    "            \"\"\"if(epoch + 1) % 100 == 0:\n",
    "                f = open(\"prova_.txt\", \"a\")\n",
    "                f.write(str(nw_l.tolist())+\"\\n\")\n",
    "                f.write(output_sequence+\"\\n\")\n",
    "                f.close() \n",
    "        \"\"\"\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "            #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "            #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "            #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'vocadataset' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m trainset_ \u001b[39m=\u001b[39m vocadataset(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, landmark\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mouthOnly\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#landmark, labels = trainset[0]\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m trainset_ \u001b[39m=\u001b[39m trainset_\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     14\u001b[0m trainloader_ \u001b[39m=\u001b[39m DataLoader(trainset_, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, collate_fn\u001b[39m=\u001b[39mcollate_fn, num_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39m###########à\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'vocadataset' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "###########\n",
    "\n",
    "#trainset = vocadataset(\"train\", landmark=True)\n",
    "trainset_ = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "#landmark, labels = trainset[0]\n",
    "trainset_ = trainset_.to(device)\n",
    "trainloader_ = DataLoader(trainset_, batch_size=1, collate_fn=collate_fn, num_workers=8)\n",
    "\n",
    "\n",
    "###########à"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 36*3\n",
    "INPUT_DIM = 36*3\n",
    "HID_DIM = 64\n",
    "output_dim = len(vocabulary)\n",
    "\n",
    "model = only_Decoder2(INPUT_DIM, HID_DIM, 2, len(vocabulary)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([243, 36, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(trainset_[0][0].shape[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 3.023833990097046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m i  \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m     landmarks \u001b[39m=\u001b[39m trainset_[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device) \n\u001b[1;32m     21\u001b[0m     label_t \u001b[39m=\u001b[39m char_to_index_batch(trainset_[i][\u001b[39m1\u001b[39m], vocabulary)\n\u001b[1;32m     22\u001b[0m     label_t \u001b[39m=\u001b[39m label_t\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/vocaset.py:202\u001b[0m, in \u001b[0;36mvocadataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m vertex, label\n\u001b[1;32m    201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     landmark \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetLandmark(vertex, index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtype)\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmouthonly \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         landmark \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetOnlyMouthlandmark(landmark)\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/vocaset.py:156\u001b[0m, in \u001b[0;36mvocadataset.getLandmark\u001b[0;34m(self, vertex, index, type)\u001b[0m\n\u001b[1;32m    151\u001b[0m landmarks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(size\u001b[39m=\u001b[39m[vertex\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m68\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vertex\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m--> 156\u001b[0m     landmarks[i] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy( get_landmarks(vertex[i],v) )\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m(i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    158\u001b[0m         \u001b[39m#landmarks[i] = torch.pow(landmarks[0]-landmarks[i],2)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m         landmarks[i] \u001b[39m=\u001b[39m landmarks[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39mlandmarks[i]\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/getlandmark.py:64\u001b[0m, in \u001b[0;36mget_landmarks\u001b[0;34m(vertices, template)\u001b[0m\n\u001b[1;32m     62\u001b[0m template_mesh \u001b[39m=\u001b[39m trimesh\u001b[39m.\u001b[39mload(template, process\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m faces\u001b[39m=\u001b[39mtemplate_mesh\u001b[39m.\u001b[39mfaces\n\u001b[0;32m---> 64\u001b[0m total_lmks\u001b[39m=\u001b[39mload_dynamic_contour(vertices, faces, contour_embeddings_path\u001b[39m=\u001b[39;49mcontour_embeddings_path, static_embedding_path\u001b[39m=\u001b[39;49mstatic_embedding_path, angle\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(angle))\n\u001b[1;32m     65\u001b[0m \u001b[39m########## Add process Landmarks\u001b[39;00m\n\u001b[1;32m     66\u001b[0m total_lmks\u001b[39m=\u001b[39mprocess_landmarks(total_lmks)\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/getlandmark.py:31\u001b[0m, in \u001b[0;36mload_dynamic_contour\u001b[0;34m(vertices, faces, contour_embeddings_path, static_embedding_path, angle)\u001b[0m\n\u001b[1;32m     29\u001b[0m lmk_face_idx_dynamic \u001b[39m=\u001b[39m dynamic_lmks_embeddings[\u001b[39m'\u001b[39m\u001b[39mlmk_face_idx\u001b[39m\u001b[39m'\u001b[39m][angle]\n\u001b[1;32m     30\u001b[0m lmk_b_coords_dynamic \u001b[39m=\u001b[39m dynamic_lmks_embeddings[\u001b[39m'\u001b[39m\u001b[39mlmk_b_coords\u001b[39m\u001b[39m'\u001b[39m][angle]\n\u001b[0;32m---> 31\u001b[0m dynamic_lmks \u001b[39m=\u001b[39m mesh_points_by_barycentric_coordinates(vertices, faces, lmk_face_idx_dynamic, lmk_b_coords_dynamic)\n\u001b[1;32m     32\u001b[0m static_lmks \u001b[39m=\u001b[39m mesh_points_by_barycentric_coordinates(vertices, faces, lmk_face_idx_static, lmk_b_coords_static)\n\u001b[1;32m     33\u001b[0m total_lmks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([dynamic_lmks, static_lmks])\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/getlandmark.py:22\u001b[0m, in \u001b[0;36mmesh_points_by_barycentric_coordinates\u001b[0;34m(mesh_verts, mesh_faces, lmk_face_idx, lmk_b_coords)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmesh_points_by_barycentric_coordinates\u001b[39m(mesh_verts, mesh_faces, lmk_face_idx, lmk_b_coords):\n\u001b[1;32m     18\u001b[0m     \u001b[39m# function: evaluation 3d points given mesh and landmark embedding\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m# modified from https://github.com/Rubikplayer/flame-fitting/blob/master/fitting/landmarks.py\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     dif1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([(mesh_verts[mesh_faces[lmk_face_idx], \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m lmk_b_coords)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     21\u001b[0m                     (mesh_verts[mesh_faces[lmk_face_idx], \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m lmk_b_coords)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m---> 22\u001b[0m                     (mesh_verts[mesh_faces[lmk_face_idx], \u001b[39m2\u001b[39;49m] \u001b[39m*\u001b[39;49m lmk_b_coords)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)])\u001b[39m.\u001b[39mT\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m dif1\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/venv/lib/python3.10/site-packages/torch/_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "###########\n",
    "trainset_ = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i  in range(10):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        landmarks = trainset_[i][0].to(device) \n",
    "        \n",
    "        label_t = char_to_index_batch(trainset_[i][1], vocabulary)\n",
    "        label_t = label_t.to(device)\n",
    "        target_tensor = label_t\n",
    "        reshaped_landmark = torch.reshape(landmarks, (landmarks.shape[0], landmarks.shape[1]*landmarks.shape[2]))\n",
    "        reshaped_landmark = reshaped_landmark.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        len_landmark = torch.tensor(landmarks.shape[0]).to(device) \n",
    "        output = model(reshaped_landmark[None, : , :], len_landmark[None])\n",
    "        output = output.permute(1,0,2)\n",
    "        input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.int32)\n",
    "        \n",
    "        loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), target_tensor.squeeze(), input_lengths, target_lengths[0][None])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "        if(epoch + 1) % 100 == 0:\n",
    "            f = open(\"prova_.txt\", \"a\")\n",
    "            f.write(label[i]+\"\\n\")\n",
    "            f.write(output_sequence+\"\\n\")\n",
    "            f.close() \n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/model20sent_3.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb22b1e55b1fbd8f3cffd3928edc0da66604f23d4dbbba430356986a4eac359a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
