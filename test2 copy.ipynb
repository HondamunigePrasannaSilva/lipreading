{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype of lipreading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# model file, encoder, decoder and seqtoseq\n",
    "from model_temp import *\n",
    "# utils file\n",
    "from utils import *\n",
    "# Get landmark using vocadataset.py\n",
    "from data.vocaset import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get landmark from vocadaset class\n",
    "#trainset = vocadataset(\"train\", landmark=True)\n",
    "trainset = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "#landmark, labels = trainset[0]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=20, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "#landmarks, len_landmark, label, len_label = next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(blank=\"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from characters to indices\n",
    "char_to_index = {char: index for index, char in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequence and target to indices\n",
    "#sequence_indices = [char_to_index[char] for char in sequence]\n",
    "\n",
    "label_t = char_to_index_batch(label, vocabulary)\n",
    "\n",
    "#target_indices = [char_to_index[char] for char in labels]\n",
    "#target_tensor = torch.tensor(target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 36*3\n",
    "INPUT_DIM = 36*3\n",
    "HID_DIM = 64\n",
    "output_dim = len(vocabulary)\n",
    "\n",
    "model = only_Decoder2(INPUT_DIM, HID_DIM, 2, len(vocabulary)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i in range(landmarks.shape[0]):\n",
    "        landmarks[i] = landmarks[i].to(device) \n",
    "        label_t[i] = label_t[i].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        target_tensor = label_t[i]\n",
    "\n",
    "        \n",
    "\n",
    "        reshaped_landmark = torch.reshape(landmarks[i], (landmarks[i].shape[0], landmarks[i].shape[1]*landmarks[i].shape[2]))\n",
    "        reshaped_landmark = reshaped_landmark.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        len_landmark[i] = len_landmark[i].to(device) \n",
    "        output = model(reshaped_landmark[None, : , :], len_landmark[i][None])\n",
    "        output = output.permute(1,0,2)\n",
    "        input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.int32)\n",
    "        \n",
    "        loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), target_tensor, input_lengths, target_lengths[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "        if(epoch + 1) % 100 == 0:\n",
    "            f = open(\"prova_.txt\", \"a\")\n",
    "            f.write(label[i]+\"\\n\")\n",
    "            f.write(output_sequence+\"\\n\")\n",
    "            f.close() \n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for landmarks, len_landmark, label, len_label in trainloader:\n",
    "    #landmarks, len_landmark, label, len_label = next(iter(trainloader))\n",
    "    label_t = char_to_index_batch(label, vocabulary)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for i in range(landmarks.shape[0]):\n",
    "            landmarks[i] = landmarks[i].to(device) \n",
    "            label_t[i] = label_t[i].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            #remove padding from dataloader\n",
    "            nw = landmarks[i][:len_landmark[i], :, :].clone().detach().requires_grad_(True)\n",
    "            nw_l = label_t[i][:len_label[i]].clone().detach()\n",
    "            \n",
    "            target_tensor = nw_l\n",
    "            reshaped_landmark = torch.reshape(nw, (nw.shape[0], nw.shape[1]*nw.shape[2]))\n",
    "            reshaped_landmark = reshaped_landmark.to(device)\n",
    "            target_tensor = target_tensor.to(device)\n",
    "            len_landmark[i] = len_landmark[i].to(device) \n",
    "            output = model(reshaped_landmark[None, : , :], len_landmark[i][None])\n",
    "            output = output.permute(1,0,2)\n",
    "            input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "            target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.int32)\n",
    "            \n",
    "            loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), target_tensor, input_lengths, target_lengths[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            e = torch.argmax(output, dim=2).squeeze(1)\n",
    "            output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "            #print(output_sequence)\n",
    "            \"\"\"if(epoch + 1) % 100 == 0:\n",
    "                f = open(\"prova_.txt\", \"a\")\n",
    "                f.write(str(nw_l.tolist())+\"\\n\")\n",
    "                f.write(output_sequence+\"\\n\")\n",
    "                f.close() \n",
    "        \"\"\"\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "            #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "            #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "            #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "###########\n",
    "\n",
    "#trainset = vocadataset(\"train\", landmark=True)\n",
    "trainset_ = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "#landmark, labels = trainset[0]\n",
    "trainset_ = trainset_.to(device)\n",
    "trainloader_ = DataLoader(trainset_, batch_size=1, collate_fn=collate_fn, num_workers=8)\n",
    "\n",
    "\n",
    "###########Ã "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 36*3\n",
    "INPUT_DIM = 36*3\n",
    "HID_DIM = 64\n",
    "output_dim = len(vocabulary)\n",
    "\n",
    "model = only_Decoder2(INPUT_DIM, HID_DIM, 2, len(vocabulary)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(trainset_[0][0].shape[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "###########\n",
    "trainset_ = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i  in range(10):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        landmarks = trainset_[i][0].to(device) \n",
    "        \n",
    "        label_t = char_to_index_batch(trainset_[i][1], vocabulary)\n",
    "        label_t = label_t.to(device)\n",
    "        target_tensor = label_t\n",
    "        reshaped_landmark = torch.reshape(landmarks, (landmarks.shape[0], landmarks.shape[1]*landmarks.shape[2]))\n",
    "        reshaped_landmark = reshaped_landmark.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        len_landmark = torch.tensor(landmarks.shape[0]).to(device) \n",
    "        output = model(reshaped_landmark[None, : , :], len_landmark[None])\n",
    "        output = output.permute(1,0,2)\n",
    "        input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.int32)\n",
    "        \n",
    "        loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), target_tensor.squeeze(), input_lengths, target_lengths[0][None])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "        if(epoch + 1) % 100 == 0:\n",
    "            f = open(\"prova_.txt\", \"a\")\n",
    "            f.write(label[i]+\"\\n\")\n",
    "            f.write(output_sequence+\"\\n\")\n",
    "            f.close() \n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valset = vocadataset(\"val\", landmark=True, mouthOnly=True)\n",
    "#landmark, labels = trainset[0]\n",
    "\n",
    "valloader = DataLoader(valset, batch_size=1, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(input_string, blank= \"@\"):\n",
    "    output_string = \"\"\n",
    "    current_char = \"\"\n",
    "\n",
    "    for char in input_string:\n",
    "        if char != current_char:\n",
    "            if char == blank:\n",
    "                output_string += ''\n",
    "            else:\n",
    "                output_string += char\n",
    "                \n",
    "            current_char = char\n",
    "        else:\n",
    "            output_string += ''\n",
    "\n",
    "\n",
    "    return output_string.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/prasanna/Documents/UNIFI/Computer Graphics/LipReading/lipreading/m/model20sent_4.pt\"))\n",
    "model.eval()\n",
    "\n",
    "real_sentences = []\n",
    "pred_sentences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for landmarks, len_landmark, label, len_label in valloader:\n",
    "\n",
    "        # reshape the batch from [batch_size, frame_size, num_landmark, 3] to [batch_size, frame_size, num_landmark * 3] \n",
    "        landmarks = torch.reshape(landmarks, (landmarks.shape[0], landmarks.shape[1], landmarks.shape[2]*landmarks.shape[3]))\n",
    "        \n",
    "        #variable to recover later the target sequences\n",
    "        label_list = label\n",
    "\n",
    "        # label char to index\n",
    "        label = char_to_index_batch(label, vocabulary)\n",
    "\n",
    "        # move data to GPU!\n",
    "        landmarks = landmarks.to(device)\n",
    "        len_landmark = len_landmark.to(device)\n",
    "        label = label.to(device)\n",
    "        len_label = len_label.to(device)\n",
    "\n",
    "        output = model(landmarks, len_landmark)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # scrittura nel file del outuput e della frase originale\n",
    "\n",
    "        real_sentences, pred_sentences = write_results(len_label, label_list, output.detach(), valloader.batch_size, vocabulary, real_sentences, pred_sentences)\n",
    "\n",
    "    pred_sentences = list(map(lambda x:process_string(x),pred_sentences))\n",
    "    save_results(f\"./results/validation.txt\", real_sentences, pred_sentences, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prasanna/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def calculate_bleu(model_sentences, reference_sentences):\n",
    "    # Tokenize the sentences\n",
    "    model_sentences = [nltk.word_tokenize(sentence) for sentence in model_sentences]\n",
    "    reference_sentences = [[nltk.word_tokenize(sentence)] for sentence in reference_sentences]\n",
    "\n",
    "    # Compute BLEU score\n",
    "    smoothie = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    bleu_score = nltk.translate.bleu_score.corpus_bleu(reference_sentences, model_sentences, smoothing_function=smoothie.method1)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcideeaetved ere doneddhim oshoraf.',\n",
       " 'aisos s acuieaes fasd dsaaintinlg ig hsadid mad.',\n",
       " 'eles md fews tlrcreiaeedsasteiveed fr re emoa acolyilr oc m ae tvn rere efre forlee dm mfoformaorcdcvg.',\n",
       " 'hiadr oanniitiniit aeac woir iutol oolom jasainct?',\n",
       " 'e tebacnthndantdau s wm wshcranin ird to orraetihsoi ied aiu adrr.',\n",
       " 'ntentifertarergtegeegaegeteittrestitntttnihtniwtititnan.',\n",
       " 't th ce idteaei aaisdeatee metiietsouu.',\n",
       " 'taiturcgtsrhvi fis oi str faratisto onraenivin itctrot.',\n",
       " 'hasirohtegdeits tocrinolg coiovnenvgveinl aegegildi  chaotedeaettdtcc.',\n",
       " 'd manai oem o on nsrgngbakh cehci wctliescoart imoghioft?',\n",
       " 'tanuotsdechi ietcedstueitcthihi odm sarhrdstrs ihsfocth tt.',\n",
       " 'intts rtntaiemtsat rtrairnianiaoarti foc oooooofsth.',\n",
       " 'actngstc ththe ot scvschchidtkog fmcle wal ?',\n",
       " 'tnetgoirtsr aauocsesoalecororicsfss.',\n",
       " 'rsomistri raed fed ussasnnsietivetcd.',\n",
       " 'hrtve oeatyisgiseacgih ethn arotactritataes st avothe g?',\n",
       " 'ttowm hein hae tasdsd esd snicuieaheaestestvi trcm aeidhe.',\n",
       " 'a  i bntnken t hae o oaaesantschesaadteseannnatid fosd ts atnsnteil hoastictiaoc flst?',\n",
       " 'warshstttfoaostaeshtee.',\n",
       " 'avdeenintsntt fi aruahreicuirdreow.',\n",
       " 'rnete aantnnsreh me faiaatht penn.',\n",
       " 'hi acarhtncestety fsd or oarcelchvm m d dshaerty intseteycaswons iooo?',\n",
       " 'ed hv fomaloticmohstsefe msogym.',\n",
       " 'anmras e cangs mrsd r antculng in os olim mo o  ch sh eacihoeorol agio o w.',\n",
       " 'nsteaasiololaourirairyolt iae.',\n",
       " 'anrpoies hcvued sve gion ertnracv ot.',\n",
       " 'eairoagis fis fohsts od aschcihcgacrai ictiic c',\n",
       " 'aleoanusstcnthsaainmsc inhsroewhochoc is?',\n",
       " 'tltnagananisaaiadd giis aathaho owffa.',\n",
       " 'harbisgregcnategiic ccoheaeoetses d fose oscserls ros?',\n",
       " 'ray vesftollve m de e eheednengiettites n heeedngfos  h o ?',\n",
       " 'tron mwaenbtetne rd aestenve fn fam saveyeane e  fo mhenede e r m  imetcoisps  d?',\n",
       " 'i ieranoe dsbaerae re ua ic sire vedagvee re reteededterdesdedtdedee.',\n",
       " 'anintcnng noarinyertremitcvd wcbhactect hsshradi w  t?',\n",
       " 'rthesainacheaestnct fotso w fof tlrrhassseegtvitvedieh.',\n",
       " 'wee hiraagrin aorneinnitrtatntntintint ntaern?',\n",
       " 'heaee oar sorre it asly aioirrn sajaigi w t.',\n",
       " 'rcaacoaredaiendhstendieae r arearede s l bhaeccosuls m.',\n",
       " 'biamu aniorwninthnt fofstt.',\n",
       " 'cttete shg hrtne stnstht srinteintintn tands ii tgeissstgiesmooo?',\n",
       " 'ahirmwid syd e aspewarded tecsosy  p oiu ist o ap orth.',\n",
       " 'ttttc o l  po fo f  a ied sbrnn e thoaned w t be ceijrv.',\n",
       " 'hbl srrrtos  roermanedaanedttns  aovecnr r tt ttto bib.',\n",
       " 'ttotmbrr ts  fr fr fr   t r   eaagn m lr  e bsos.',\n",
       " 'tthkswsatnt tate teeailaheestesotheeeeee me aneted tm wod apedte fo slorvgvnoohe ae.',\n",
       " 'taeimueen ed frm  r et  mee.',\n",
       " 'oas hestre ietevgs f fo s poajeuaes o fos m atosaa bhfe.',\n",
       " 'o s mtaen m ftoiuntsaelgsuae d f pc wf o e pets te bops.',\n",
       " 'nntnte mo r o fr fr fttrb om n re o e e hts f ft ahaem ieehee .',\n",
       " 'tmmaatrruyliand shasme onmer tved te ft tene  fr  rr r o t b?',\n",
       " 'aaa wost  tten r  fm arnm r fos d sesurn r toeef.',\n",
       " 'taitenannshstyhvas ea n  sas rynse medeads w  al s evoplsm fi cuunon.',\n",
       " 'tttsmaaism mhaedte  ve  nthicanss e.',\n",
       " 'hhathi n ree iesm ere otom fteasn m m man m b m .',\n",
       " 'betetned f r  m s e  fr m aer oemahv frmoes m d hal.',\n",
       " 'ags f otnimbhah id ti o amoue f f a fro  fre meynd fr .',\n",
       " 'eraned tndenererrts  yed uind emeg e.',\n",
       " 'red he tws yladecanbrhvs w sha nd hehoys ataihethe.',\n",
       " 'iec nd m f emhvetf ft d heg t.',\n",
       " 'httcl ed aaemsmabteemee nom atos om dnciehethe.',\n",
       " 'rcnanuiessun ins f eom bdealin en wmon aaaprhiecvees nd he.',\n",
       " 'ecnnen emd diemneidkg iajem ywlgsmalin walyain tn ag fsooiorhile?',\n",
       " 'thioyioy as d s aee c ceve rsr  fro n ctns rearhhvntstnace t.',\n",
       " 'ahirmwid syd e aspewarded tecsosy  p oiu ist o ap orth.',\n",
       " 'aonatensced cf ors from losass tind hetct in m egvoi',\n",
       " 'taatestcli ctiwsaeam m rtoeclrastktfsts tefuf d wm asnd e whee eaavm bmoie rbhe.',\n",
       " 'tee o e eyd aecos m aere ea rm esn anasurainded tesas shes evavdm fer gsaed atiemeg .',\n",
       " 'rieoao eto damosd md  h n r p ros   .',\n",
       " 'asinuds mngcs hroaoeon wo   ircieo c ke?',\n",
       " 'ahmanc cmdchpenynnactey ey wlcn aes hayio mt mtocoe to.',\n",
       " 'araa tri am  ara hveae n en etnitarn e reedaneesdmm.',\n",
       " 'toe mpysed ny e ed d yde   e tc  otc ouuts.',\n",
       " 'tmew wdeomlatett ehetm fod aharom  rrmonafromlipeerm  heithe e wee   e woaorsecsmmmb.',\n",
       " 'amae ndesa s ncn er ten d fes nensdesjeasn sats f td maishe n smlews seact ns inyditend eere .',\n",
       " 'ndlae mt or renh  w  atve fefraewe ve.',\n",
       " 'e m  hcmcos  ftf tmtbeehe m fer  s ss os  fta a e d efhr.',\n",
       " 'ttoam br   abm ao  lod rsnd e e arete hemvche ts  f ro meitenteos  rts es o ftmptavag.',\n",
       " 'tanntiansnglnaet f snyfancuoskok lae  hfrt nsasnnsno  ni sasetede f o frtlhorb?',\n",
       " 'm ih  ahe e r t tde f  hted mmvted.',\n",
       " 'rettacytatcthieo fo tb ies fd anas rans on t obva.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bleu(['Ciao mi chiamo prasanna'],['Ciao mi chiamo prasanna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/model20sent_4.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 (main, Apr 20 2023, 19:02:41) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e60b588e56b1bd547ea8afbc4b0f4ab4b79ce7cedb5d172a4ce68e35ca79034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
