{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype of lipreading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "# model file, encoder, decoder and seqtoseq\n",
    "from model_temp import *\n",
    "# utils file\n",
    "from utils import *\n",
    "# Get landmark using vocadataset.py\n",
    "from data.vocaset import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get landmark from vocadaset class\n",
    "#trainset = vocadataset(\"train\", landmark=True)\n",
    "trainset = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "#landmark, labels = trainset[0]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=20, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "landmarks, len_landmark, label, len_label = next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = create_vocabulary(blank=\"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from characters to indices\n",
    "char_to_index = {char: index for index, char in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sequence and target to indices\n",
    "#sequence_indices = [char_to_index[char] for char in sequence]\n",
    "\n",
    "label_t = char_to_index_batch(label, vocabulary)\n",
    "\n",
    "#target_indices = [char_to_index[char] for char in labels]\n",
    "#target_tensor = torch.tensor(target_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 36*3\n",
    "INPUT_DIM = 36*3\n",
    "HID_DIM = 64\n",
    "output_dim = len(vocabulary)\n",
    "\n",
    "model = only_Decoder(INPUT_DIM, HID_DIM, 2, len(vocabulary)).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.reshape(landmarks[0], (landmarks[0].shape[0], landmarks[0].shape[1]*landmarks[0].shape[2]))[None,:,:].shape)\n",
    "print(len_landmark[0][None, None].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 1.8985503911972046\n",
      "Epoch [20/10000], Loss: 1.6644002199172974\n",
      "Epoch [30/10000], Loss: 1.5414624214172363\n",
      "Epoch [40/10000], Loss: 1.3623918294906616\n",
      "Epoch [50/10000], Loss: 1.2469836473464966\n",
      "Epoch [60/10000], Loss: 1.1365450620651245\n",
      "Epoch [70/10000], Loss: 1.0001591444015503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m e \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m output_sequence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([vocabulary[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m e])\n\u001b[1;32m     31\u001b[0m \u001b[39m#print(output_sequence)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m e \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m output_sequence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([vocabulary[index] \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m e])\n\u001b[1;32m     31\u001b[0m \u001b[39m#print(output_sequence)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i in range(landmarks.shape[0]):\n",
    "        landmarks[i] = landmarks[i].to(device) \n",
    "        label_t[i] = label_t[i].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        target_tensor = label_t[i]\n",
    "        reshaped_landmark = torch.reshape(landmarks[i], (landmarks[i].shape[0], landmarks[i].shape[1]*landmarks[i].shape[2]))\n",
    "        reshaped_landmark = reshaped_landmark.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "        len_landmark[i] = len_landmark[i].to(device) \n",
    "        output = model(reshaped_landmark[None, : , :], len_landmark[i][None])\n",
    "        output = output.permute(1,0,2)\n",
    "        input_lengths = torch.full((1,), output.size(0), dtype=torch.long)\n",
    "        target_lengths = torch.full((target_tensor.size(0),), target_tensor.size(0), dtype=torch.int32)\n",
    "        \n",
    "        loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), target_tensor, input_lengths, target_lengths[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "        if(epoch + 1) % 100 == 0:\n",
    "            f = open(\"prova_.txt\", \"a\")\n",
    "            f.write(label[i]+\"\\n\")\n",
    "            f.write(output_sequence+\"\\n\")\n",
    "            f.close() \n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "###########\n",
    "\n",
    "#trainset = vocadataset(\"train\", landmark=True)\n",
    "trainset_ = vocadataset(\"train\", landmark=True, mouthOnly=True)\n",
    "#landmark, labels = trainset[0]\n",
    "\n",
    "trainloader_ = DataLoader(trainset_, batch_size=1, collate_fn=collate_fn, num_workers=8)\n",
    "\n",
    "###########Ã "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m i, (landmarks, len_landmark, label, len_label) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m         \u001b[39m# reshape the batch from [batch_size, frame_size, num_landmark, 3] to [batch_size, frame_size, num_landmark * 3] \u001b[39;00m\n\u001b[1;32m      7\u001b[0m         landmarks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(landmarks, (landmarks\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], landmarks\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], landmarks\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\u001b[39m*\u001b[39mlandmarks\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]))\n\u001b[1;32m      9\u001b[0m         \u001b[39m#variable to recover later the target sequences\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/vocaset.py:202\u001b[0m, in \u001b[0;36mvocadataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m vertex, label\n\u001b[1;32m    201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m     landmark \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetLandmark(vertex, index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtype)\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmouthonly \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         landmark \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetOnlyMouthlandmark(landmark)\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/vocaset.py:156\u001b[0m, in \u001b[0;36mvocadataset.getLandmark\u001b[0;34m(self, vertex, index, type)\u001b[0m\n\u001b[1;32m    151\u001b[0m landmarks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(size\u001b[39m=\u001b[39m[vertex\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m68\u001b[39m, \u001b[39m3\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vertex\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m--> 156\u001b[0m     landmarks[i] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy( get_landmarks(vertex[i],v) )\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m(i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    158\u001b[0m         \u001b[39m#landmarks[i] = torch.pow(landmarks[0]-landmarks[i],2)\u001b[39;00m\n\u001b[1;32m    159\u001b[0m         landmarks[i] \u001b[39m=\u001b[39m landmarks[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39mlandmarks[i]\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/getlandmark.py:64\u001b[0m, in \u001b[0;36mget_landmarks\u001b[0;34m(vertices, template)\u001b[0m\n\u001b[1;32m     62\u001b[0m template_mesh \u001b[39m=\u001b[39m trimesh\u001b[39m.\u001b[39mload(template, process\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m faces\u001b[39m=\u001b[39mtemplate_mesh\u001b[39m.\u001b[39mfaces\n\u001b[0;32m---> 64\u001b[0m total_lmks\u001b[39m=\u001b[39mload_dynamic_contour(vertices, faces, contour_embeddings_path\u001b[39m=\u001b[39;49mcontour_embeddings_path, static_embedding_path\u001b[39m=\u001b[39;49mstatic_embedding_path, angle\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(angle))\n\u001b[1;32m     65\u001b[0m \u001b[39m########## Add process Landmarks\u001b[39;00m\n\u001b[1;32m     66\u001b[0m total_lmks\u001b[39m=\u001b[39mprocess_landmarks(total_lmks)\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/lipreading/data/getlandmark.py:27\u001b[0m, in \u001b[0;36mload_dynamic_contour\u001b[0;34m(vertices, faces, contour_embeddings_path, static_embedding_path, angle)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dynamic_contour\u001b[39m(vertices, faces, contour_embeddings_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m, static_embedding_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m, angle\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m     26\u001b[0m     contour_embeddings_path \u001b[39m=\u001b[39m contour_embeddings_path\n\u001b[0;32m---> 27\u001b[0m     dynamic_lmks_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(contour_embeddings_path, allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlatin1\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m     lmk_face_idx_static, lmk_b_coords_static \u001b[39m=\u001b[39m load_static_embedding(static_embedding_path)\n\u001b[1;32m     29\u001b[0m     lmk_face_idx_dynamic \u001b[39m=\u001b[39m dynamic_lmks_embeddings[\u001b[39m'\u001b[39m\u001b[39mlmk_face_idx\u001b[39m\u001b[39m'\u001b[39m][angle]\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/venv/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/Documents/UNIFI/Computer Graphics/LipReading/venv/lib/python3.10/site-packages/numpy/lib/format.py:792\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    790\u001b[0m     pickle_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    791\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 792\u001b[0m     array \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(fp, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_kwargs)\n\u001b[1;32m    793\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    794\u001b[0m     \u001b[39m# Friendlier error message\u001b[39;00m\n\u001b[1;32m    795\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mUnicodeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnpickling a python object failed: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mYou may need to pass the encoding= option \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mto numpy.load\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (err,)) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (landmarks, len_landmark, label, len_label) in enumerate(trainloader):\n",
    "\n",
    "        # reshape the batch from [batch_size, frame_size, num_landmark, 3] to [batch_size, frame_size, num_landmark * 3] \n",
    "        landmarks = torch.reshape(landmarks, (landmarks.shape[0], landmarks.shape[1], landmarks.shape[2]*landmarks.shape[3]))\n",
    "        \n",
    "        #variable to recover later the target sequences\n",
    "        label_list = label\n",
    "\n",
    "        # label char to index\n",
    "        label = char_to_index_batch(label, vocabulary)\n",
    "\n",
    "        # move data to GPU!\n",
    "        landmarks = landmarks.to(device)\n",
    "        len_landmark = len_landmark.to(device)\n",
    "        label = label.to(device)\n",
    "        len_label = len_label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(landmarks,len_landmark )\n",
    "        output = output.permute(1, 0, 2)#had to permute for the ctc loss. it acceprs [seq_len, batch_size, \"num_class\"]\n",
    "\n",
    "        loss = ctc_loss(torch.nn.functional.log_softmax(output, dim=2), label, len_landmark, len_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \"\"\"e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "        if(epoch + 1) % 1 == 0:\n",
    "            f = open(\"prova_.txt\", \"a\")\n",
    "            f.write(label[i]+\"\\n\")\n",
    "            f.write(output_sequence+\"\\n\")\n",
    "            f.close() \"\"\"\n",
    "        print(i)\n",
    "        if(i == 10):\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        #e = torch.argmax(output, dim=2).squeeze(1)\n",
    "        #output_sequence = ''.join([vocabulary[index] for index in e])\n",
    "        #print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"models/model20sent_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the output sequence\n",
    "output_indices = torch.argmax(output, dim=2).squeeze(1)\n",
    "output_sequence = ''.join([vocabulary[index] for index in output_indices])\n",
    "\n",
    "print(\"Target Sequence:\", labels.replace(\"@\",\"\").replace(\"#\",\"\"))\n",
    "print(\"Decoded Output:\", process_string(output_sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb22b1e55b1fbd8f3cffd3928edc0da66604f23d4dbbba430356986a4eac359a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
